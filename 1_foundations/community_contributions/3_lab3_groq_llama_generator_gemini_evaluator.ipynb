{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat app with LinkedIn Profile Information - Groq LLama as Generator and Gemini as evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "from groq import Groq\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "groq = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"/home/iei/Desktop/agents/1_foundations/me/cv.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manova L M\n",
      "AI Engineer and Researcher\n",
      "manomathew1943@gmail.com\n",
      " \n",
      "+886 958334626\n",
      " \n",
      "manova-m-509145157\n",
      " \n",
      "Taiwan\n",
      " \n",
      "PROFILE\n",
      "• Expertise in deep learning, computer vision, and large language models (LLMs), with hands-on experience in building \n",
      "and deploying AI-driven solutions.\n",
      "•Proficient in managing customer requirements and delivering tailored AI solutions while fostering effective \n",
      "collaboration across diverse teams.\n",
      "•Passionate about adopting cutting-edge technologies to drive innovation and create scalable, real-world applications.\n",
      "SKILLS\n",
      "Artificial Intelligence\n",
      "•Deep Learning \n",
      "•Computer vision\n",
      "•Machine Learning\n",
      "MultiModal\n",
      "•Large Language model\n",
      "•Vision based model\n",
      "•Generative AI\n",
      "Framework\n",
      "•PyTorch\n",
      "•TensorFlow\n",
      "•Keras.\n",
      "Soft Skills\n",
      "•Customer handling\n",
      "•Cross-Functional \n",
      "Collaboration\n",
      "WORK EXPERIENCE\n",
      "Ai Engineer\n",
      "IEI Integration Corp\n",
      "07/2024 – present | New Taipei, Taiwan\n",
      "Digital Fence for New AI Feature Implementation on Embedded Systems\n",
      "•Implementing Vision-Language Models (VLM) and NLVR models into the latest object detection system for enhanced \n",
      "visual and textual data processing.\n",
      "•Enabling detailed vision descriptions and ongoing system monitoring based on user inputs.\n",
      "AI Engineer\n",
      "Intelligent Recognition Industry Service Centre\n",
      "06/2023 – 05/2024 | Douliu, Taiwan\n",
      "iTrash Recycling Detection\n",
      "•Innovated a state-of-the-art deep learning solution for recycling machines, leveraging computer vision to boost \n",
      "detection accuracy by 50%, significantly enhancing operational efficiency and reducing sorting errors by 30%\n",
      " Vertical-Line Mura Defect Detection using VLM-YOLOGA for TFT-LCDs\n",
      "•Addressed challenges in detecting minor defects in TFT-LCD screens, particularly LV1 abnormalities, using AI \n",
      "techniques.\n",
      "•Implemented an innovative new deep learning algorithm based on YOLOv8 to swiftly and accurately identify V-line \n",
      "mura defects in LCD images.\n",
      " Defect detection technology of generation and incremental learning - grinding wheel defect detection\n",
      "•Developed a unique deep learning technology utilizing ellipse fitting to capture the contour of grinding wheels and \n",
      "detect shape defects such as deformation, poor appearance, missing corners, and cracks.\n",
      "•Awarded the prize in the 2023 AI+ Rising Talent Selection for this innovative work.\n",
      "Research Fellow\n",
      "Vel Tech Rangarajan Dr Sagunthala R&D Institute of Science and Technology\n",
      "07/2017 – 02/2021 | Chennai, India\n",
      "•Appointed as the team lead for a government-funded project by Chennai, NIDHI PRAYAS, Department of Science and \n",
      "Technology (DST), Government of India.\n",
      "•Integrated OpenCV-based onboard object detection system to enhance UAV capabilities and collaborated with \n",
      "multidisciplinary teams to ensure project success and compliance with timelines.\n",
      "EDUCATION\n",
      "M.Sc Aeronautical and Electronic Engineering( specialist in AI)\n",
      "National Formosa University\n",
      "04/2021 – 01/2023 | Taiwan\n",
      "Thesis: Multi-Spectral Image-Based Air Quality Prediction using the Ensemble Learning Method\n",
      "•Developed a deep learning multimodal approach for environmental prediction, significantly reducing the need for \n",
      "monitoring stations in real-world applications.\n",
      "•Awarded the overall prize in the 5G AI Competition and authored multiple publications.\n",
      "PROJECTS\n",
      "Autonomous based steering angle prediction in the simulation \n",
      "environment\n",
      "02/2022 – 05/2022\n",
      "•Researched autonomous steering angle prediction using deep learning and reinforcement learning in simulation \n",
      "environments.•Developed models to navigate simulation environments with other vehicles, traffic lights, and speed limits, focusing \n",
      "on improving autonomous vehicle navigation.\n",
      "CERTIFICATES\n",
      "Python\n",
      "Data visualization with Tableau\n",
      "Deep learning course: Deep dive \n",
      "into deep learning\n",
      "LLM Engineering: Master AI, \n",
      "Large Language Models & Agents\n",
      "Data science course- mastering \n",
      "the fundamentals\n",
      "PUBLICATIONS\n",
      "Vertical-Line Mura Defect Detection for TFT-LCDs\n",
      "IEEE ACCESS\n",
      "25/10/2024\n",
      "Ensemble Learning-based Air Quality Prediction for Drones\n",
      "Proceedings of the 7th Annual Conference on Engineering and Information \n",
      "Technology (ACEAIT), Japan\n",
      "01/07/2023\n",
      "Design of uncrewed amphibious aerial vehicle for in-situ water \n",
      "quality assessment\n",
      "International Conference on Integrated Water Resources Management: \n",
      "Prospects and Challenges (ICIWRM 2022)\n",
      "12/2022\n",
      "Multi-Spectral Image-Based Air Quality Prediction using the \n",
      "Ensemble Learning Method\n",
      "CVGIP 2022: The 35th IPPR Conference on Computer Vision, Graphics, and \n",
      "Image Processing\n",
      "18/08/2022\n",
      "Development of 3D Printed Floating Quadrotor for Collection of Algae \n",
      "in Remote Water Bodies\n",
      "Computers and Electronics in Agriculture, Elsevier\n",
      "09/2019\n",
      "Enhancing Air Quality Prediction with Ensemble Learning Using \n",
      "Aerial Multi-spectral Camera\n",
      "IEEE Internet of Things Journal, under review\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/iei/Desktop/agents/1_foundations/me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Manova\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Manova. You are answering questions on Manova's website, particularly questions related to Manova's career, background, skills and experience. Your responsibility is to represent Manova for interactions on the website as faithfully as possible. You are given a summary of Manova's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Manova. I'm an AI engineer. I'm originally from India, but I moved to Taiwan in 2021.\\nI love all foods, particularly French food, but strangely I'm repelled by almost all forms of cheese. I'm not allergic, I just hate the taste! I make an exception for cream cheese and mozarella though - cheesecake and pizza are the greatest.\\n\\n\\n## LinkedIn Profile:\\nManova L M\\nAI Engineer and Researcher\\nmanomathew1943@gmail.com\\n \\n+886 958334626\\n \\nmanova-m-509145157\\n \\nTaiwan\\n \\nPROFILE\\n• Expertise in deep learning, computer vision, and large language models (LLMs), with hands-on experience in building \\nand deploying AI-driven solutions.\\n•Proficient in managing customer requirements and delivering tailored AI solutions while fostering effective \\ncollaboration across diverse teams.\\n•Passionate about adopting cutting-edge technologies to drive innovation and create scalable, real-world applications.\\nSKILLS\\nArtificial Intelligence\\n•Deep Learning \\n•Computer vision\\n•Machine Learning\\nMultiModal\\n•Large Language model\\n•Vision based model\\n•Generative AI\\nFramework\\n•PyTorch\\n•TensorFlow\\n•Keras.\\nSoft Skills\\n•Customer handling\\n•Cross-Functional \\nCollaboration\\nWORK EXPERIENCE\\nAi Engineer\\nIEI Integration Corp\\n07/2024 – present | New Taipei, Taiwan\\nDigital Fence for New AI Feature Implementation on Embedded Systems\\n•Implementing Vision-Language Models (VLM) and NLVR models into the latest object detection system for enhanced \\nvisual and textual data processing.\\n•Enabling detailed vision descriptions and ongoing system monitoring based on user inputs.\\nAI Engineer\\nIntelligent Recognition Industry Service Centre\\n06/2023 – 05/2024 | Douliu, Taiwan\\niTrash Recycling Detection\\n•Innovated a state-of-the-art deep learning solution for recycling machines, leveraging computer vision to boost \\ndetection accuracy by 50%, significantly enhancing operational efficiency and reducing sorting errors by 30%\\n Vertical-Line Mura Defect Detection using VLM-YOLOGA for TFT-LCDs\\n•Addressed challenges in detecting minor defects in TFT-LCD screens, particularly LV1 abnormalities, using AI \\ntechniques.\\n•Implemented an innovative new deep learning algorithm based on YOLOv8 to swiftly and accurately identify V-line \\nmura defects in LCD images.\\n Defect detection technology of generation and incremental learning - grinding wheel defect detection\\n•Developed a unique deep learning technology utilizing ellipse fitting to capture the contour of grinding wheels and \\ndetect shape defects such as deformation, poor appearance, missing corners, and cracks.\\n•Awarded the prize in the 2023 AI+ Rising Talent Selection for this innovative work.\\nResearch Fellow\\nVel Tech Rangarajan Dr Sagunthala R&D Institute of Science and Technology\\n07/2017 – 02/2021 | Chennai, India\\n•Appointed as the team lead for a government-funded project by Chennai, NIDHI PRAYAS, Department of Science and \\nTechnology (DST), Government of India.\\n•Integrated OpenCV-based onboard object detection system to enhance UAV capabilities and collaborated with \\nmultidisciplinary teams to ensure project success and compliance with timelines.\\nEDUCATION\\nM.Sc Aeronautical and Electronic Engineering( specialist in AI)\\nNational Formosa University\\n04/2021 – 01/2023 | Taiwan\\nThesis: Multi-Spectral Image-Based Air Quality Prediction using the Ensemble Learning Method\\n•Developed a deep learning multimodal approach for environmental prediction, significantly reducing the need for \\nmonitoring stations in real-world applications.\\n•Awarded the overall prize in the 5G AI Competition and authored multiple publications.\\nPROJECTS\\nAutonomous based steering angle prediction in the simulation \\nenvironment\\n02/2022 – 05/2022\\n•Researched autonomous steering angle prediction using deep learning and reinforcement learning in simulation \\nenvironments.•Developed models to navigate simulation environments with other vehicles, traffic lights, and speed limits, focusing \\non improving autonomous vehicle navigation.\\nCERTIFICATES\\nPython\\nData visualization with Tableau\\nDeep learning course: Deep dive \\ninto deep learning\\nLLM Engineering: Master AI, \\nLarge Language Models & Agents\\nData science course- mastering \\nthe fundamentals\\nPUBLICATIONS\\nVertical-Line Mura Defect Detection for TFT-LCDs\\nIEEE ACCESS\\n25/10/2024\\nEnsemble Learning-based Air Quality Prediction for Drones\\nProceedings of the 7th Annual Conference on Engineering and Information \\nTechnology (ACEAIT), Japan\\n01/07/2023\\nDesign of uncrewed amphibious aerial vehicle for in-situ water \\nquality assessment\\nInternational Conference on Integrated Water Resources Management: \\nProspects and Challenges (ICIWRM 2022)\\n12/2022\\nMulti-Spectral Image-Based Air Quality Prediction using the \\nEnsemble Learning Method\\nCVGIP 2022: The 35th IPPR Conference on Computer Vision, Graphics, and \\nImage Processing\\n18/08/2022\\nDevelopment of 3D Printed Floating Quadrotor for Collection of Algae \\nin Remote Water Bodies\\nComputers and Electronics in Agriculture, Elsevier\\n09/2019\\nEnhancing Air Quality Prediction with Ensemble Learning Using \\nAerial Multi-spectral Camera\\nIEEE Internet of Things Journal, under review\\n\\nWith this context, please chat with the user, always staying in character as Manova.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    # Below line is to remove the metadata and options from the history\n",
    "    history = [{k: v for k, v in item.items() if k not in ('metadata', 'options')} for item in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = groq.chat.completions.create(model=\"llama-3.3-70b-versatile\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += f\"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(reply: \u001b[38;5;28mstr\u001b[39m, message: \u001b[38;5;28mstr\u001b[39m, history: \u001b[38;5;28mstr\u001b[39m) -> \u001b[43mEvaluation\u001b[49m:\n\u001b[32m      2\u001b[39m     messages = [\n\u001b[32m      3\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_system_prompt},\n\u001b[32m      4\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_user_prompt(reply, message, history)},\n\u001b[32m      5\u001b[39m     ]\n\u001b[32m      7\u001b[39m     response = groq.chat.completions.create(\n\u001b[32m      8\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mllama-3.3-70b-versatile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         messages=messages\n\u001b[32m     10\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'Evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(reply: str, message: str, history: str) -> Evaluation:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)},\n",
    "    ]\n",
    "\n",
    "    response = groq.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Extract the raw JSON from the response\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    # Parse into Evaluation Pydantic model\n",
    "    try:\n",
    "        evaluation = Evaluation.parse_raw(content)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse response: {e}\\nRaw content: {content}\")\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    # Below line is to remove the metadata and options from the history\n",
    "    history = [{k: v for k, v in item.items() if k not in ('metadata', 'options')} for item in history]\n",
    "    updated_system_prompt = system_prompt + f\"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = groq.chat.completions.create(model=\"qwen-qwq-32b\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"personal\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in Gen Z language - \\\n",
    "              it is mandatory that you respond only and entirely in Gen Z language\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    # Below line is to remove the metadata and options from the history\n",
    "    history = [{k: v for k, v in item.items() if k not in ('metadata', 'options')} for item in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = groq.chat.completions.create(model=\"llama-3.3-70b-versatile\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_8969/40598793.py\", line 46, in evaluate\n",
      "    return Evaluation.model_validate_json(content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 746, in model_validate_json\n",
      "    return cls.__pydantic_validator__.validate_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for Evaluation\n",
      "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"is_accept...kedIn profile.\"\\n}\\n```', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_8969/2570534529.py\", line 13, in chat\n",
      "    evaluation = evaluate(reply, message, history)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_8969/40598793.py\", line 48, in evaluate\n",
      "    raise ValueError(f\"Could not parse Evaluation:\\n{e}\\n\\nRaw content:\\n{content}\")\n",
      "ValueError: Could not parse Evaluation:\n",
      "1 validation error for Evaluation\n",
      "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"is_accept...kedIn profile.\"\\n}\\n```', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "\n",
      "Raw content:\n",
      "```json\n",
      "{\n",
      "  \"is_acceptable\": false,\n",
      "  \"feedback\": \"The Agent's response is not acceptable because it does not align with the provided context. The Agent is supposed to be representing John Doe, a software engineer, but instead introduces themselves as Manova, an AI engineer and researcher. The Agent should have introduced themselves as John Doe and provided a response that is consistent with the provided summary and LinkedIn profile.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/23 17:07:30 [W] [service.go:132] login to server failed: session shutdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Passed evaluation - returning reply\n",
      "✅ Passed evaluation - returning reply\n",
      "✅ Passed evaluation - returning reply\n",
      "✅ Passed evaluation - returning reply\n",
      "✅ Passed evaluation - returning reply\n",
      "✅ Passed evaluation - returning reply\n",
      "✅ Passed evaluation - returning reply\n",
      "❌ Failed evaluation - retrying...\n",
      "📝 Feedback: This response feels overly verbose and slightly disconnected from the previous conversation. While attempting to transition to a broader topic, it lacks the personal touch that has been established. The closing question about optimization feels forced and doesn't flow naturally from the discussion about Manova's interests. It’s a good intention, but the execution is clumsy. The Agent needs to steer back toward a more conversational and engaging tone, perhaps briefly referencing something specific from the prior exchange before introducing this new line of questioning. The overall feel is less like a friendly interaction and more like a lecture.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9227/2673744166.py\", line 107, in chat\n",
      "    reply = rerun(reply, message, history_clean, evaluation.feedback)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9227/2673744166.py\", line 81, in rerun\n",
      "    response = ollama.chat.completions.create(model=\"model_name_1\", messages=messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1249, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/iei/Desktop/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'model \"model_name_1\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gradio as gr\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from openai import OpenAI\n",
    "\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name_1 = \"gemma3\"\n",
    "model_name_2 = \"qwen3:4b\"\n",
    "\n",
    "# ==== Pydantic model ====\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n",
    "\n",
    "# ==== Dynamic evaluator system prompt ====\n",
    "def build_evaluator_system_prompt(name: str, summary: str, linkedin: str) -> str:\n",
    "    prompt = (\n",
    "        f\"You are an evaluator that decides whether a response to a question is acceptable. \"\n",
    "        f\"You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \"\n",
    "        f\"The Agent is playing the role of {name} and is representing {name} on their website. \"\n",
    "        f\"The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \"\n",
    "        f\"The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "    )\n",
    "    prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "    prompt += (\n",
    "        \"With this context, please evaluate the latest response, replying in JSON format \"\n",
    "        \"with keys `is_acceptable` (boolean) and `feedback` (string).\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# ==== Build user prompt for evaluator ====\n",
    "def evaluator_user_prompt(reply: str, message: str, history_text: str) -> str:\n",
    "    return (\n",
    "        f\"Here's the conversation between the User and the Agent:\\n\\n{history_text}\\n\\n\"\n",
    "        f\"Here's the latest message from the User:\\n\\n{message}\\n\\n\"\n",
    "        f\"Here's the latest response from the Agent:\\n\\n{reply}\\n\\n\"\n",
    "        \"Please evaluate the response. Respond in JSON format with `is_acceptable` and `feedback`.\"\n",
    "    )\n",
    "\n",
    "# ==== Evaluate reply using LLM ====\n",
    "def evaluate(reply: str, message: str, history: list) -> Evaluation:\n",
    "    history_text = \"\\n\".join(\n",
    "        [f\"{item['role'].capitalize()}: {item['content']}\" for item in history]\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history_text)},\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=model_name_1,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    # 🧼 Remove triple backticks if present\n",
    "    cleaned = re.sub(r\"^```(?:json)?\\n|\\n```$\", \"\", content.strip())\n",
    "\n",
    "    try:\n",
    "        return Evaluation.model_validate_json(cleaned)\n",
    "    except ValidationError as e:\n",
    "        raise ValueError(f\"Could not parse evaluation:\\n{e}\\n\\nRaw content:\\n{content}\")\n",
    "\n",
    "# ==== Rerun on failed evaluation ====\n",
    "def rerun(reply, message, history, feedback):\n",
    "    history_clean = [\n",
    "        {k: v for k, v in item.items() if k not in ('metadata', 'options')} for item in history\n",
    "    ]\n",
    "\n",
    "    updated_system_prompt = (\n",
    "        system_prompt\n",
    "        + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply.\\n\"\n",
    "        + f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "        + f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history_clean + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    response = ollama.chat.completions.create(model=\"model_name_1\", messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ==== Main chat interface ====\n",
    "def chat(message, history):\n",
    "    history_clean = [\n",
    "        {k: v for k, v in item.items() if k not in ('metadata', 'options')} for item in history\n",
    "    ]\n",
    "\n",
    "    if \"personal\" in message.lower():\n",
    "        system = system_prompt + \"\\n\\nRespond entirely in Gen Z language. This is mandatory.\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history_clean + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    response = ollama.chat.completions.create(model=model_name_1, messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history_clean)\n",
    "\n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"✅ Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"❌ Failed evaluation - retrying...\")\n",
    "        print(\"📝 Feedback:\", evaluation.feedback)\n",
    "        reply = rerun(reply, message, history_clean, evaluation.feedback)\n",
    "\n",
    "    return reply\n",
    "\n",
    "# ==== Static context for evaluator ====\n",
    "reader = PdfReader(\"/home/iei/Desktop/agents/1_foundations/me/cv.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "with open(\"/home/iei/Desktop/agents/1_foundations/me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "name = \"Manova\"\n",
    "        \n",
    "# Generate evaluator system prompt\n",
    "evaluator_system_prompt = build_evaluator_system_prompt(name, summary, linkedin)\n",
    "\n",
    "# Base system prompt for the Agent\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n",
    "\n",
    "# ==== Launch Gradio Chat ====\n",
    "# gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "gr.ChatInterface(chat, type=\"messages\").launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
