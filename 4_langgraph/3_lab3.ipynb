{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Week 4, Day 4\n",
    "\n",
    "This is the start of an AWESOME project! Really simple and very effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First - a heads up for Windows PC users\n",
    "\n",
    "While executing this notebook, you might hit a problem with the Playwright browser raising a NotImplementedError.\n",
    "\n",
    "This should work when we move to python modules, but it can cause problems in Windows in a notebook.\n",
    "\n",
    "If you it this error and would like to run the notebook, you need to make a small change which seems quite hacky!\n",
    "\n",
    "1. Right click in `.venv` in the File Explorer on the left and select \"Find in folder\"\n",
    "2. Search for `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())`  \n",
    "3. That code should be found in a line of code in a file called `kernelapp.py`\n",
    "4. Comment out that line of code in that file! And save the file. (And in fact, student William Lapa tells me that he needed to comment out the entire else statement that this line is part of.)\n",
    "5. Restart the kernel by pressing the \"Restart\" button above\n",
    "\n",
    "Thank you to student Nicolas for finding this, and to Yaki, Zibin and Bhaskar for confirming that this worked for them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "import requests\n",
    "import os\n",
    "from langchain.agents import Tool\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous LangGraph\n",
    "\n",
    "To run a tool:  \n",
    "Sync: `tool.run(inputs)`  \n",
    "Async: `await tool.arun(inputs)`\n",
    "\n",
    "To invoke the graph:  \n",
    "Sync: `graph.invoke(state)`  \n",
    "Async: `await graph.ainvoke(state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "pushover_url = \"https://api.pushover.net/1/messages.json\"\n",
    "\n",
    "def push(text: str):\n",
    "    \"\"\"Send a push notification to the user\"\"\"\n",
    "    requests.post(pushover_url, data = {\"token\": pushover_token, \"user\": pushover_user, \"message\": text})\n",
    "\n",
    "tool_push = Tool(\n",
    "        name=\"send_push_notification\",\n",
    "        func=push,\n",
    "        description=\"useful for when you want to send a push notification\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: Install Playwright\n",
    "\n",
    "On Windows and MacOS:  \n",
    "`playwright install`\n",
    "\n",
    "On Linux:  \n",
    "`playwright install â€”with-reps chromium`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing nest_asyncio\n",
    "\n",
    "Python async code only allows for one \"event loop\" processing aynchronous events.\n",
    "\n",
    "The `nest_asyncio` library patches this, and is used for special situations, if you need to run a nested event loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LangChain community\n",
    "\n",
    "One of the remarkable things about LangChain is the rich community around it.\n",
    "\n",
    "Check this out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "\n",
    "# If you get a NotImplementedError here or later, see the Heads Up at the top of the notebook\n",
    "\n",
    "async_browser =  create_async_playwright_browser(headless=False)  # headful mode\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "tools = toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in tools:\n",
    "    print(f\"{tool.name}={tool}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_dict = {tool.name:tool for tool in tools}\n",
    "\n",
    "navigate_tool = tool_dict.get(\"navigate_browser\")\n",
    "extract_text_tool = tool_dict.get(\"extract_text\")\n",
    "\n",
    "    \n",
    "await navigate_tool.arun({\"url\": \"https://www.cnn.com\"})\n",
    "text = await extract_text_tool.arun({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "print(textwrap.fill(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = tools + [tool_push]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:120b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")   # or \"mistral\", \"codellama\", etc.\n",
    "\n",
    "llm_with_tools = llm.bind_tools(all_tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=all_tools))\n",
    "graph_builder.add_conditional_edges( \"chatbot\", tools_condition, \"tools\")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "async def chat(user_input: str, history):\n",
    "    result = await graph.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATOR RESULT: {'score': 10, 'reasoning': 'The answer correctly states that Paris is the capital of France, which is accurate and complete.'}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Define LLM (Ollama model)\n",
    "# ----------------------------\n",
    "# Make sure you have `ollama serve` running and model pulled (e.g. `ollama pull llama3`)\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:120b\",   # change to the model you have locally, like \"mistral\", \"llama3.1\", etc.\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Define schema (expected JSON fields)\n",
    "# ----------------------------\n",
    "schema = {\n",
    "    \"score\": \"An integer score between 1 and 10\",\n",
    "    \"reasoning\": \"A short explanation for the score\"\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Prompt Template\n",
    "# ----------------------------\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an evaluator.\n",
    "Return ONLY valid JSON with two fields:\n",
    "- score: an integer from 1 (very poor answer) to 10 (perfectly correct answer).\n",
    "- reasoning: a short explanation of why you gave that score.\n",
    "Do not include extra text.\"\"\"),\n",
    "    (\"user\", \"Evaluate the following answer:\\n\\n{answer}\")\n",
    "])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Parser with auto-fix\n",
    "# ----------------------------\n",
    "parser = OutputFixingParser.from_llm(\n",
    "    llm=llm,\n",
    "    parser=SimpleJsonOutputParser()\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Full pipeline (prompt -> llm -> parser)\n",
    "# ----------------------------\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Run evaluator\n",
    "# ----------------------------\n",
    "async def main():\n",
    "    test_answer = \"The capital of France is Paris.\"\n",
    "    result = await chain.ainvoke({\"answer\": test_answer})\n",
    "    print(\"EVALUATOR RESULT:\", result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "import gradio as gr\n",
    "import uuid\n",
    "import nest_asyncio\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------------------\n",
    "# Load environment\n",
    "# -------------------------------\n",
    "load_dotenv(override=True)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# -------------------------------\n",
    "# Structured evaluator output\n",
    "# -------------------------------\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user\")\n",
    "\n",
    "# -------------------------------\n",
    "# State definition\n",
    "# -------------------------------\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    success_criteria: str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool\n",
    "\n",
    "# -------------------------------\n",
    "# Playwright & tools setup\n",
    "# -------------------------------\n",
    "async_browser = create_async_playwright_browser(headless=False)\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "# -------------------------------\n",
    "# Ollama LLM setup\n",
    "# -------------------------------\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "worker_llm = ChatOllama(model=\"gpt-oss:120b\", temperature=0)\n",
    "worker_llm_with_tools = worker_llm.bind_tools(tools)\n",
    "\n",
    "evaluator_llm = ChatOllama(model=\"gpt-oss:120b\", temperature=0)\n",
    "\n",
    "# -------------------------------\n",
    "# Safe evaluator wrapper\n",
    "# -------------------------------\n",
    "def extract_json_safe(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract JSON from LLM output or fallback to default\"\"\"\n",
    "    try:\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match:\n",
    "            return json.loads(match.group(0))\n",
    "    except:\n",
    "        pass\n",
    "    return {\n",
    "        \"feedback\": \"Evaluator could not parse LLM output. Needs user input.\",\n",
    "        \"success_criteria_met\": False,\n",
    "        \"user_input_needed\": True\n",
    "    }\n",
    "\n",
    "def safe_evaluator_invoke(messages: List[Any]) -> EvaluatorOutput:\n",
    "    \"\"\"Call evaluator LLM and safely parse JSON\"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an evaluator. ONLY output valid JSON with keys: \"\n",
    "        \"feedback (string), success_criteria_met (bool), user_input_needed (bool). \"\n",
    "        \"Do NOT include any extra text or markdown.\"\n",
    "    )\n",
    "    evaluator_messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    raw_output = evaluator_llm.invoke(evaluator_messages)\n",
    "    data = extract_json_safe(raw_output.content)\n",
    "    return EvaluatorOutput(**data)\n",
    "\n",
    "# -------------------------------\n",
    "# Worker node\n",
    "# -------------------------------\n",
    "def worker(state: State) -> Dict[str, Any]:\n",
    "    system_msg = f\"You are a helpful assistant. Complete the task or ask the user a question.\\nSuccess criteria:\\n{state['success_criteria']}\"\n",
    "    if state.get(\"feedback_on_work\"):\n",
    "        system_msg += f\"\\nFeedback from previous attempt:\\n{state['feedback_on_work']}\"\n",
    "    messages = [SystemMessage(content=system_msg)] + state[\"messages\"]\n",
    "    response = worker_llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def worker_router(state: State) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"evaluator\"\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluator node\n",
    "# -------------------------------\n",
    "def evaluator(state: State) -> State:\n",
    "    last_response = state[\"messages\"][-1].content\n",
    "    user_msg = HumanMessage(content=f\"Conversation:\\n{last_response}\")\n",
    "    eval_result = safe_evaluator_invoke([user_msg])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Evaluator Feedback: {eval_result.feedback}\")],\n",
    "        \"feedback_on_work\": eval_result.feedback,\n",
    "        \"success_criteria_met\": eval_result.success_criteria_met,\n",
    "        \"user_input_needed\": eval_result.user_input_needed,\n",
    "    }\n",
    "\n",
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state[\"success_criteria_met\"] or state[\"user_input_needed\"]:\n",
    "        return \"END\"\n",
    "    return \"worker\"\n",
    "\n",
    "# -------------------------------\n",
    "# Graph setup\n",
    "# -------------------------------\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"worker\", worker)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"worker\", worker_router, {\"tools\": \"tools\", \"evaluator\": \"evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"worker\")\n",
    "graph_builder.add_conditional_edges(\"evaluator\", route_based_on_evaluation, {\"worker\": \"worker\", \"END\": END})\n",
    "graph_builder.add_edge(START, \"worker\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# -------------------------------\n",
    "# Gradio helpers\n",
    "# -------------------------------\n",
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "async def process_message(message, success_criteria, history, thread):\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"success_criteria\": success_criteria,\n",
    "        \"feedback_on_work\": None,\n",
    "        \"success_criteria_met\": False,\n",
    "        \"user_input_needed\": False,\n",
    "    }\n",
    "    result = await graph.ainvoke(state, config={\"configurable\": {\"thread_id\": thread}})\n",
    "    user = {\"role\": \"user\", \"content\": message}\n",
    "    reply = {\"role\": \"assistant\", \"content\": result[\"messages\"][-2].content}\n",
    "    feedback = {\"role\": \"assistant\", \"content\": result[\"messages\"][-1].content}\n",
    "    return history + [user, reply, feedback]\n",
    "\n",
    "async def reset():\n",
    "    return \"\", \"\", None, make_thread_id()\n",
    "\n",
    "# -------------------------------\n",
    "# Gradio UI\n",
    "# -------------------------------\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"emerald\")) as demo:\n",
    "    gr.Markdown(\"## Sidekick Personal Co-worker\")\n",
    "    thread = gr.State(make_thread_id())\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"Sidekick\", height=300, type=\"messages\")\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            message = gr.Textbox(show_label=False, placeholder=\"Your request to your sidekick\")\n",
    "        with gr.Row():\n",
    "            success_criteria = gr.Textbox(show_label=False, placeholder=\"What are your success criteria?\")\n",
    "    with gr.Row():\n",
    "        reset_button = gr.Button(\"Reset\", variant=\"stop\")\n",
    "        go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "\n",
    "    message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    reset_button.click(reset, [], [message, success_criteria, chatbot, thread])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker LLM failed: registry.ollama.ai/library/deepseek-r1:70b does not support tools (status code: 400)\n",
      "Worker LLM failed: registry.ollama.ai/library/deepseek-r1:70b does not support tools (status code: 400)\n",
      "Worker LLM failed: registry.ollama.ai/library/deepseek-r1:70b does not support tools (status code: 400)\n",
      "Worker LLM failed: registry.ollama.ai/library/deepseek-r1:70b does not support tools (status code: 400)\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "import gradio as gr\n",
    "import uuid\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# -------------------------------\n",
    "# Structured output for evaluator\n",
    "# -------------------------------\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user, or clarifications\")\n",
    "\n",
    "# -------------------------------\n",
    "# Define state\n",
    "# -------------------------------\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    success_criteria: str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool\n",
    "\n",
    "# -------------------------------\n",
    "# Setup Playwright tools\n",
    "# -------------------------------\n",
    "async_browser = create_async_playwright_browser(headless=False)\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "# -------------------------------\n",
    "# Setup Ollama LLMs\n",
    "# -------------------------------\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "worker_llm = ChatOllama(model=\"llama3.3\", temperature=0)\n",
    "worker_llm_with_tools = worker_llm.bind_tools(tools)\n",
    "\n",
    "evaluator_llm = ChatOllama(model=\"llama3.3\", temperature=0)\n",
    "evaluator_llm_with_output = evaluator_llm.with_structured_output(EvaluatorOutput)\n",
    "\n",
    "# -------------------------------\n",
    "# Worker node\n",
    "# -------------------------------\n",
    "def worker(state: State) -> Dict[str, Any]:\n",
    "    system_message = f\"\"\"You are a helpful assistant that can use tools to complete tasks.\n",
    "You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\n",
    "This is the success criteria:\n",
    "{state['success_criteria']}\n",
    "You should reply either with a question for the user or with the final answer.\n",
    "\"\"\"\n",
    "\n",
    "    if state.get(\"feedback_on_work\"):\n",
    "        system_message += f\"\"\"\n",
    "Previously your response was rejected because the success criteria was not met:\n",
    "{state['feedback_on_work']}\n",
    "Please continue, ensuring that you meet the success criteria or ask a question for the user.\n",
    "\"\"\"\n",
    "\n",
    "    # Update or insert system message\n",
    "    messages = state[\"messages\"]\n",
    "    found_system = False\n",
    "    for i, msg in enumerate(messages):\n",
    "        if isinstance(msg, SystemMessage):\n",
    "            messages[i].content = system_message\n",
    "            found_system = True\n",
    "            break\n",
    "    if not found_system:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "\n",
    "    # Invoke LLM with tools safely\n",
    "    try:\n",
    "        response = worker_llm_with_tools.invoke(messages)\n",
    "    except Exception as e:\n",
    "        print(f\"Worker LLM failed: {e}\")\n",
    "        response = AIMessage(content=\"I'm unable to process the request. Can you clarify?\")\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# -------------------------------\n",
    "# Worker router\n",
    "# -------------------------------\n",
    "def worker_router(state: State) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"evaluator\"\n",
    "\n",
    "# -------------------------------\n",
    "# Format conversation helper\n",
    "# -------------------------------\n",
    "def format_conversation(messages: List[Any]) -> str:\n",
    "    conversation = \"Conversation history:\\n\\n\"\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            conversation += f\"User: {msg.content}\\n\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            conversation += f\"Assistant: {msg.content or '[Tool used]'}\\n\"\n",
    "    return conversation\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluator node (safe)\n",
    "# -------------------------------\n",
    "def evaluator(state: State) -> State:\n",
    "    last_response = state[\"messages\"][-1].content\n",
    "\n",
    "    system_msg = \"You are an evaluator that determines if the assistant's task is completed successfully.\"\n",
    "    user_msg = f\"\"\"Conversation:\n",
    "{format_conversation(state['messages'])}\n",
    "\n",
    "Success criteria:\n",
    "{state['success_criteria']}\n",
    "\n",
    "Assistant's last response:\n",
    "{last_response}\n",
    "\"\"\"\n",
    "\n",
    "    if state[\"feedback_on_work\"]:\n",
    "        user_msg += f\"\\nNote: Prior feedback was: {state['feedback_on_work']}\"\n",
    "\n",
    "    messages = [SystemMessage(content=system_msg), HumanMessage(content=user_msg)]\n",
    "\n",
    "    try:\n",
    "        eval_result = evaluator_llm_with_output.invoke(messages)\n",
    "        feedback = eval_result.feedback\n",
    "        success_met = eval_result.success_criteria_met\n",
    "        user_needed = eval_result.user_input_needed\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluator LLM failed: {e}\")\n",
    "        feedback = \"Evaluator could not parse output.\"\n",
    "        success_met = False\n",
    "        user_needed = True\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Evaluator Feedback: {feedback}\")],\n",
    "        \"feedback_on_work\": feedback,\n",
    "        \"success_criteria_met\": success_met,\n",
    "        \"user_input_needed\": user_needed\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Route evaluator based on result\n",
    "# -------------------------------\n",
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state[\"success_criteria_met\"] or state[\"user_input_needed\"]:\n",
    "        return \"END\"\n",
    "    return \"worker\"\n",
    "\n",
    "# -------------------------------\n",
    "# Build graph\n",
    "# -------------------------------\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"worker\", worker)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"worker\", worker_router, {\"tools\": \"tools\", \"evaluator\": \"evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"worker\")\n",
    "graph_builder.add_conditional_edges(\"evaluator\", route_based_on_evaluation, {\"worker\": \"worker\", \"END\": END})\n",
    "graph_builder.add_edge(START, \"worker\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers for Gradio\n",
    "# -------------------------------\n",
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "async def process_message(message, success_criteria, history, thread):\n",
    "    config = {\"configurable\": {\"thread_id\": thread}}\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"success_criteria\": success_criteria,\n",
    "        \"feedback_on_work\": None,\n",
    "        \"success_criteria_met\": False,\n",
    "        \"user_input_needed\": False,\n",
    "    }\n",
    "\n",
    "    result = await graph.ainvoke(state, config=config)\n",
    "    user_msg = {\"role\": \"user\", \"content\": message}\n",
    "    reply_msg = {\"role\": \"assistant\", \"content\": result[\"messages\"][-2].content}\n",
    "    feedback_msg = {\"role\": \"assistant\", \"content\": result[\"messages\"][-1].content}\n",
    "\n",
    "    return history + [user_msg, reply_msg, feedback_msg]\n",
    "\n",
    "async def reset():\n",
    "    return \"\", \"\", None, make_thread_id()\n",
    "\n",
    "# -------------------------------\n",
    "# Gradio UI\n",
    "# -------------------------------\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"emerald\")) as demo:\n",
    "    gr.Markdown(\"## Sidekick Personal Co-worker\")\n",
    "    thread = gr.State(make_thread_id())\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"Sidekick\", height=300, type=\"messages\")\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            message = gr.Textbox(show_label=False, placeholder=\"Your request to your sidekick\")\n",
    "        with gr.Row():\n",
    "            success_criteria = gr.Textbox(show_label=False, placeholder=\"What are your success criteria?\")\n",
    "    with gr.Row():\n",
    "        reset_button = gr.Button(\"Reset\", variant=\"stop\")\n",
    "        go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "\n",
    "    message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    reset_button.click(reset, [], [message, success_criteria, chatbot, thread])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
